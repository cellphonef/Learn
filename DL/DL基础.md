

# DL

DL，即Deep Learning=深度学习。

**What**

在人工智能领域里，有一个方法叫机器学习，而机器学习里面有一类算法叫神经网络，而深度学习就是使用深层神经网络进行学习的方法。


![DL](img/DL基础_2021-09-16-10-34-12.png)




## 数字图像基础

对于黑白图像在计算机中是一堆按照顺序排列的数字，取值范围[0, 255]。0表示最暗，255表示最亮，这些数字又称为灰度值。

对于彩色图像，其颜色是由三原色（红R，绿G，蓝B）组合形成。



## 数学基础

### 极限

邻域即某个点附近的区域

> **极限定义**
>
> 
 
极限的四则运算法则

若 $\lim f(x) = A，\lim g(x) = B，$ 则
- $\lim \left[f(x) \pm g(x) \right] = A + B$
- $\lim f(x) \cdot g(x)  = A \cdot B$
- $\lim \frac{f(x)}{g(x)} = \frac{A}{B} (B \neq 0)$

### 一元函数

**连续**

1.在一点处连续

如果 $\lim\limits_{x \rightarrow a} f(x) = f(a)$，函数 $f$ 在点 $x=a$ 处连续。

解析：
- 函数f(x)在x=a的某邻域内有定义。
- 函数f在x=a极限值存在。
- x=a处的极限等于f(a)。

2.在一个区间上连续

区间上连续分为开区间连续和闭区间连续。

总结：
- 每一个多项式函数都是连续的。
- 连续函数的四则运算或者复合还是连续的。


**可导**

可导必连续，连续不一定可导。




### 二元函数

### 推广：多元函数


### 方向导数与梯度

『导数』是函数的局部性质，一个函数在某一点的导数描述了这个函数在这一点附近的变化率。

『方向导数』顾名思义就是某个方向上的导数。

对于一元函数 $y = f(x)$，其自变量为一元，因此只有一个方向的方向导数，即沿着x轴方向的导数（因为x只能增加或减少）。

而对于多元函数，如二元函数 $z = f(x, y)$，其自变量为二元，因此可以朝着一个平面的任意方向进行改变，因此有无数个方向导数。

在这无数个方向导数中，有两个特殊方向（即沿着坐标轴x和y轴）的方向导数称为偏导数。












**梯度下降法**

BGD（Batch Gradient Descent，批量梯度下降法）

SGD（Stochastic Gradient Descent，随机梯度下降法）

MBGD（Mini-Batch Gradient Descent，小批量梯度下降法）


## 计算图



## 神经网络

神经网络如下图所示：

![NN](img/DL基础_2021-09-16-10-36-20.png)

图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。

一个神经网络的架构分层一般分为三层：
- 最左边的输入层（Input Layer, 1层）
- 中间的隐藏层（Hidden Layer, >=1层）
- 最右边的输出层（Output Layer, 1层）

隐藏层比较多的神经网络称为深度神经网络。

**神经元**

神经网络的基本组成结构是神经元。

一个神经元的结构如下图：

![神经元](img/DL基础_2021-09-16-10-50-46.png)

其组成部分有：
- 输入权值（Input & Weight）。一个神经元可以接受多个输入，每个输入上都有一个权值，此外还有一个bias偏置项。
- 激活函数（activate function）。激活函数有多重选择。
- 输出。每个神经元都会产生一个输出，可能作为下一层神经元的输入。输出的计算方式 $y = f(w * x + b)$

> Q：[为什么要有激活函数？](https://www.zhihu.com/question/22334626)
> 
> A：激活函数是用来加入非线性因素的，解决线性模型不能解决的问题。


目标函数

损失函数

优化算法

我们的目标是使得目标函数最优，每次根据样本的标签和预测计算损失，然后求得目标函数，根据优化算法进行优化。



实际上根据神经元的不同连接方式有不同的神经网络：
- 卷积神经网络
- 循环神经网络
- 长短时记忆神经网络
- 递归神经网络




## CNN——卷积神经网络




feature map的尺寸计算方式： 
- $(input\_size + 2 * padding\_size - filter_size) / stride + 1$

把filter看成特征提取器，由神经网络自己训练，越深的网络提取的特征越深越具有一般性。




## RNN——循环神经网络


## LSTM——长短时记忆神经网络


## 递归神经网络






## 名词概念

**Ground Truth**
- 译为：地面实况。在机器学习中就是『参考标准』，预测结果根据该参考标准计算损失，并优化该损失，在有监督学习中，可以简单理解成标签。

例如目标检测情景中：

![Ground Truth](img/DL基础_2021-09-15-20-27-06.png)



**iteration**
- 一个iteration（中译：迭代），每次迭代更新一次网络结构的参数。

**batch-size**
- 一次迭代所使用的样本称为batch，其数量称为batch-size。


**epoch**

- 一个epoch（中译：轮次）指的是遍历完训练集里的每一个样本，即跑完所有batch。
- 多个epoch的目的是为了寻找全局最低值，因为一个epoch可能到不了全局最低值（参见：[Why are multiple epochs needed for deep learning?](https://www.quora.com/Why-are-multiple-epochs-needed-for-deep-learning)）。
- 每次epoch通常伴随着shuffle（参见：[浅谈深度学习shuffle问题](https://blog.csdn.net/g_b_l/article/details/109600536)）。


**regularization**
- 译为：正则化。用于防止过拟合。
- 主要有两种手段：
  - L2正则化。
  - Dropout正则化。


**normalization**
- 译为：归一化。用于消除不同数据之间的量纲，方便数据比较和共同处理。
    - 线性转换：$y = \frac{x - min}{max - min}$
    - 非线性转换：如对数转换和反余切函数转换。

**gradient vanish& explode**
- 译为：梯度消失和梯度爆炸
- 梯度消失是指训练过程中，梯度值几乎消失（变得很小），使得权重无法得到有效更新
- 梯度爆炸是指训练过程中，梯度值呈现指数增长。


## 答疑

