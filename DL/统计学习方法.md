# 统计学习方法概论

统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。

监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。换句话说，学习的目的就在于找到最好的这样的模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是**假设空间**。假设空间的确定意味着学习范围的确定。

实现统计学习方法的步骤如下：
1. 得到一个有限的训练数据集合。
2. 确定包含所有可能的模型的假设空间，即学习模型的集合。
3. 确定模型选择的准则，即学习的策略。
4. 实现求解最优模型的算法，即学习的算法。
5. 通过学习方法选择最优模型。
6. 利用学习的最优模型对新数据进行预测或分析。


## 统计学习方法三要素

统计学习方法都是由**模型**、**策略**和**算法**构成的（后面所有的方法都由这三部分构成）。

统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了。

### 模型

**统计学习首要考虑的问题是学习什么样的模型（人为确定，e.g. 线性模型）。** 在监督学习过程中，模型就是所要学习的条件概率分布（$P(Y|X)$）或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间的模型一般有无穷多个。

### 策略

**有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。** 统计学习的目标在于从假设空间中选取最优模型。

#### 损失函数和风险函数

- 损失函数（loss Function），也称为代价函数（cost function）是度量模型一次预测的好坏的指标。
- 风险函数度量的是平均意义下模型预测的好坏。






学习的目标就是选择期望风险最小的模型。由于联合分布PXY是未知的，RF不能直接计算。实际上，如果知道PXY，可以从联合分布直接求出条件概率分布PYX，也就不需要学习了。正因为不知道联合概率分布，所以才需要进行学习。这样一来，一方面根据期望风险最小学习模型要用到联合概率分布，另一方面联合分布又是未知的，所以监督学习就成了一个病态问题。

根据大数定律，当样本容量N趋于无穷时，经验风险RF趋于期望风险Rf。所以一个很自然的想法时是用经验风险估计期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不是很理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。

#### 经验风险最小化和结构风险最小化

在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式就可以确定。**根据经验风险最小化的策略，经验风险最小的模型就是最优的模型。**

当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。

但是当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合”现象。结构风险最小化时为了防止过拟合而提出来的策略。结构风险最小化等价于正则化。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。

其中Jf为模型的复杂度，时定义在假设空间上的泛函。模型f越复杂，复杂度Jf就越大，反之模型f越简单，复杂度Jf就越小（理解：模型越复杂表达能力越强，越可能过拟合）。$\lambda 0$ 是系数，用以权衡经验风险和模型复杂度。结构风险需要经验风险和模型复杂度同时小，结构风险小的模型通常对训练数据以及未知的测试数据都有较好的预测。

**结构风险最小化策略人为结构风险最小的模型就是最优的模型。**

变成最优化问题。

> 损失函数如果是参数的连续可导函数则更易于优化。

### 算法

**算法是指学习模型的具体计算方法。** 统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法来求解最优模型。

**统计学习的算法称为求解最优化问题的算法。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要的问题。**

> 算法应该是可收敛的否则将无实际意义。





## 模型


### 感知机

感知机是**二元分类**的**线性**分类模型：
- 输入为实例的特征向量
- 输出为实例的类别，取值+1和-1二值。

感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。

**几何解释**

感知机有如下几何解释：线性方程
$$w * x + b = 0$$
对应于特征空间$R^n$中的一个超平面S，其中w是超平面的法向量，b是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正、负两类。·

### K近邻


### 朴素贝叶斯法

#### 先验概率和后验概率

自知乎：

- 先验概率是以全事件为背景下，A事件发生的概率，即 $P(A | \Omega)$。全事件一般统计得到，所以称为先验概率，没有实验前的概率。
- 后验概率是以新事物B为背景下，A事件发生的概率，即 $P(A | B)$。新事件一般是实验，该事件B的发生与A有关联，也即A的概率会改变，由于是事件发生后的A的概率，所以称为后验概率。

### 逻辑回归

### SVM——支持向量机




## 附录

### 损失函数

#### 信息熵

信息熵是**衡量随机变量分布的混乱程度**（信息熵越大，表示越混乱则说明输出的不确定性越高），是随机分布各事件发生的信息量的期望值。当随机变量服从均匀分布时，熵最大（理解：每个概率都一样，不确定性很大，相反，当概率差距很大，则确定性很大）。

信息熵的数学定义:
$$H(X) = -\sum_{x}p(x)logp(x) = -\sum_{i=1}^{n}p(x_i)logp(x_i)$$

由定义可见，信息熵只依赖于X的分布，而不依赖于X的取值，所以也可将X的熵记作 $H(p)$.


#### 联合熵




#### 相对熵，KL散度

相对熵可以**用来衡量两个概率分布之间的差异**。

相对熵的数学定义，设p为真实分布，q为预测分布：
$$D_{KL}(p || q) = \sum_{x}p(x)log\frac{p(x)}{q(x)} = \sum_{x}p(x)(log(p(x))-log(q(x))) \\ 
 = \sum_{x}p(x)log(p(x))-\sum_{x}p(x)log(q(x))\\
 = -\sum_{x}p(x)log(q(x))-(-\sum_{x}p(x)log(p(x)))\\
 = H(p,q) - H(p)
$$

相对熵的性质：
- $D_{KL}(p||q) \geq 0 \quad 当且仅当p==q时为0$ （可由吉布斯不等式证明）
- $D_{KL}(p||q) \neq D_{KL}(p || q) \quad只有p==q时取等号$


Note：在神经网络中，我们将ground truth看成真实分布p，将predictions看成预测分布q。

#### 交叉熵

交叉熵是使用非真实分布q(x)来表示真实分布p(x)。

交叉熵的数学定义：
$$H(p, q) = -\sum_{x}p(x)logq(x)$$



参考链接：
- [详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)
- [一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉](https://cloud.tencent.com/developer/article/1162139)


### 极大似然估计

极大似然估计，通俗来讲，**就是利用已知样本结果信息，反推最具有可能导致这些样本结果出现的模型的参数值！（从现实世界照进理想世界）**

重要前提：**所有取样都是独立同分布的！**



参考链接：
- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)


### 最优化问题

最优化问题可以分为两类：
- 无约束最优化问题（梯度下降法、牛顿法和拟牛顿法）
- 有约束最优化问题（拉格朗日对偶性）

#### 梯度下降法


个人理解：
- 梯度下降算法能够往下降方向的原理可以参考泰勒公式。


#### 牛顿法和拟牛顿法


#### 拉格朗日对偶性

在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过解对偶问题而得到原始问题的解。

参考链接：
- [简易解说拉格朗日对偶](https://www.cnblogs.com/90zeng/p/Lagrange_duality.html)



#### 凸优化问题

凸优化定义：如果一个最优化问题的可行域是**凸集**，并且目标函数是**凸函数**，则该问题为凸优化问题。

对于一般的最优化问题，梯度下降法和牛顿法通过导数来进行优化，找到的都是导数或梯度为0的点，而梯度等于0的点只是取得全局极值的必要条件而不是充分条件，可能找到的极值点是局部极值而不是全局极值，另外还有可能找到的是鞍点连极值都不是。如果能使梯度为0的点是取得全局极值的充分条件，就能保证最优化算法能找到全局极值，而在凸优化问题中，梯度为0的点必然是全局极值。

Note：神经网络优化的目标函数通常不是凸函数，因此有陷入局部最小值和鞍点的风险（解决方法是采用更聪明的梯度下降法）。


参考链接：
- [理解凸优化](https://zhuanlan.zhihu.com/p/37108430)