# DL外的领域知识

## 语义分割


## 生成对抗网络（GAN）

GAN的基本原理很简单，以生成图片为例。假设我们有两个网络：G（Genrator）和D（Discriminator）：
- G是生成图片的网络，它接受一个随机噪声z，通过这个噪声生成图片，记作G(z)。
- D是判别网络，判别一张图片是不是真实的，它的输入为x，x代表一张图片，输出D(x)代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。

在训练过程中，生成网络G的目标是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实图片区分开来（给真实图片很高的置信度，而假图片很低的置信度）。G和D构成了一个动态博弈过程。

最后博弈的结果，最理想条件下，G可以生成足以以假乱真的图片G(z)。对于D来说，它难以判断G生成的图片究竟是不是真的，因此D(G(z))=0.5。

用数学语言来描述就是：


训练细节：

![GAN](img/DL外领域知识_2021-11-04-10-04-09.png)

G和D是交替训练的，并且优化的方向和东西都不一样。


## 注意力机制（Attention）

按注意力机制的可微性，可分为：
- Hard-Attention，即0/1问题，某个区域要么被关注，要么不关注，不可微。
- Soft-Attention，[0, 1]连续分布问题，用0到1的不同权重表示每个区域被关注程度的高低，可微。


按注意力机制的关注域，可分为：
- 空间域spatial domain
- 通道域channel domain
- 层域layer domain
- 混合域mixed domain
- 时间域time domain

### CNN with Attention

主要分为两种，一种是spatial attention，另外一种是channel attention。


Attention机制听起来高大上，其实就是学习出一种权重映射图，再拿这个权重映射图加在原来的特征之上，就叫Attention。

Q：为什么仅仅通过矩阵相乘，就可以建模像素和全局的联系。

A：我们熟知的图像分类网络，最后有一个FC层，目的是做矩阵的乘法，如果对应位置得到一个很大的值，说明属于这一类的概率很高，矩阵相乘本质也是向量相乘。那么对于自注意力。如果大家仔细阅读，会发现相乘的这两个矩阵本身reshape的方式不一样。他们之间的矩阵相乘，内部其实是一个像素对应的特征向量和其他所有位置的特征向量的乘积。self attention其实是想通过点积（dot）的方式，优化向量的距离，即点积相似性：如果两个向量的乘积越大，姑且可以认为这两个向量在特征空间的分布距离很近，则它们很相似。以此来建模像素之间的联系。









## 熵

### 信息熵

信息熵是**衡量随机变量分布的混乱程度**（越不混乱则说明输出越准确），是随机分布各事件发生的信息量的期望值，随机变量的取值个数越多，状态数也就越多，信息熵就越大。当随机变量服从均匀分布时，熵最大。

信息熵的数学定义:
$$H(X) = -\sum_{x}p(x)logp(x) = -\sum_{i=1}^{n}p(x_i)logp(x_i)$$

### 相对熵，KL散度

相对熵可以**用来衡量两个概率分布之间的差异**，但不是距离（因为KL散度不满足对称性）。

相对熵的数学定义：设
$$D_{KL}(p || q) = \sum_{x}p(x)log\frac{p(x)}{q(x)}$$




### 交叉熵

交叉熵是使用非真实分布q(x)来表示真实分布p(x)。

交叉熵的数学定义：
$$H(p, q) = -\sum_{x}p(x)logq(x)$$






参考链接：
- [详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)






