# CS231

# Introduction

## Image Classification


困难与挑战：
- Viewpoint variation（视角变化）.
- Scale variatioin（尺寸变化）.
- Deformation（形状变化）.
- Occlusion（遮挡）.
- （光照条件）
- backgound clutter（背景干扰）
- intra-class variation（类内差异）.




### 用于超参数调优的验证集

对于分类器而言，通常包含需要超参数，这个超参数不是训练得到的，而是人为设置的，并且设置的好坏也十分影响模型的性能。虽然超参数的设置很重要，但是并没有显而易见的设置方法，只能通过尝试的方法来决定超参数的取值。而测试某个超参数的值是否更高效的方式就是采用验证集来测试性能。

使用验证集来调优超参数的思路：从训练集中取出一部分数据当作验证集来调优，然后使用不同的超参数对验证集进行测试，选择效果最好的超参数，来测试真正的测试数据。

交叉验证。

特别注意：**绝不能使用测试集来进行调优**。当你在设计机器学习算法时，应该把测试集当作宝贵的资源，不到最后一步绝不使用它。如果你使用测试集来调优，而且算法性能看起来不错，这将给你带了一种假象，实际上当你算法实际部署后，性能可能会远远低于预期，因为**过拟合**。你的模型学习到的可能不是泛化能力，而是拟合测试集的能力。



## 反向传播

每个门单元都会得到一些input并可以立即计算两个东西：
1. 这个门的output。
2. 以及该output对于各个input的梯度。

门单元完成这两件事是完全独立的，它不需要知道计算图中的其他细节。

当前向传播完成，开始反向传播，门单元将最终获得整个计算图的loss对于该门output的梯度值。根据链式法则，门单元将回传的梯度乘以它对其input的局部梯度，从而得到整个计算图的loss对于各个input的梯度。



对前向传播的变量进行缓存。

在不同分支的梯度应该累加。


标量对向量求导 $y = f(\bold{x})$，得到一个向量。

$$\frac{\partial{y}}{\partial{\bold{x}}} =  

\left[
\begin{matrix} 
    \frac{\partial{y}}{\partial{x_1}} \\
    \\
    \frac{\partial{y}}{\partial{x_2}} \\
    \\
    \vdots \\
    \\
    \frac{\partial{y}}{\partial{x_n}} \\

\end{matrix}
\right]
$$


向量对向量求导 $\bold{y} = f(\bold{x})$，得到一个矩阵。

$$\frac{\partial{\bold{y}}}{\partial{\bold{x}}} =  

\left[
\begin{matrix} 
    \frac{\partial{y_1}}{\partial{x_1}} & \frac{\partial{y_2}}{\partial{x_1}} & \cdots &\frac{\partial{y_m}}{\partial{x_1}}\\
    \\
    \frac{\partial{y_1}}{\partial{x_2}} & \frac{\partial{y_2}}{\partial{x_2}} & \cdots & \frac{\partial{y_m}}{\partial{x_2}} \\
    \\
    \vdots & \vdots & \ddots & \vdots \\
    \\
    \frac{\partial{y_1}}{x_n} & \frac{\partial{y_2}}{x_n} & \cdots & \frac{\partial{y_m}}{x_n} \\

\end{matrix}
\right]
$$


矩阵对矩阵的求导...


一个求导简明扼要的解释：

个人理解，其实无论什么对什么求导，首先它得有它的数学意义而不是上来就是公式。不管是矩阵也好，向量也罢，假如A对B求导，A中有m个元素（如果是矩阵就是a\*b=m,向量就是m维向量，m为1就是单变量，甚至可以是张量，a\*b\*c=m,a\*b\*c\*d=m等等）,B中有n个元素。那么A对B求导就应该有m\*n个元素，即A中每个元素对B中每个元素的偏导数。现在有了这些个元素，至于这些元素要以什么样的形式摆放（就如你到处都能看到的教材或资料中的什么单变量对向量求导是行向量，向量对单变量求导是列向量，向量对向量求导是矩阵等等），其实就是要找到一种符合直觉的利于计算的（比如在矩阵／向量求导中便于使用chain rule计算导数等）统一的定义就行。刚好矩阵本身就定义了一种很符合需求的算子，所以就有了你看到的各种矩阵求导所定义的形式。（其实你如果不满意，你大可自己尝试定义一种对上述m\*n个元素的放置方式，使得你的定义能满足你的一切计算需求。不过我相信你很难找到一种比目前定义更好的定义方式）

作者：Gary
链接：https://www.zhihu.com/question/39523290/answer/221129395







## 神经网络


### 常用激活函数


**Sigmoid**。

然而现在sigmoid函数已经不太受欢迎，实际很少使用了，因为它有两个主要的缺点：
- sigmoid函数饱和使得梯度消失。
- sigmoid函数的输出不是零中心的。


**Tanh**。


**ReLU**。




## 神经网络训练

### 数据预处理



### 权重初始化

在开始训练网络之前，还需要初始化网络的权重参数。



### 批量归一化（Batch Normalization）

### 正则化（Regularization）

正则化的作用是防止过拟合。

主要手段有：
- L1/L2正则化
- Maxnorm正则化
- Dropout正则化


Dropout可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数。





